---
title: "Cars Listing Predictions"
author: "Chris Chang,Ting Hsaun Chen,Yifang He,Mekhal Raj,Mohammed AlQenae,Haiyuan Zhang"
date: "10/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup & Loading Data
```{r message = FALSE, warning = FALSE}
library(data.table)
library(ggplot2)
library(ggthemes)
library(glmnet)
library(scales) 
library(rpart) 
library(rpart.plot)
library(fastDummies)
library(randomForest)
theme_set(theme_bw())

cars_data <- fread("D:/MSBA/BA 810/cars_sample.csv")
```
## Clean Data

### 1. Dataset Overview
Overview for car listing features

```{r}
summary(cars_data)
```
### 2. Replace NA
```{r}
cars_data <- data.table(cars_data)
#dropping columns
cars_data = subset(cars_data, select = -c(dateCrawled, seller, offerType, abtest,monthOfRegistration,dateCreated,postalCode,lastSeen,name))

#removing NAs
lapply(cars_data,function(x) { length(which(is.na(x)))})
cars_data = na.omit (cars_data, cols="fuelType")
```
```{r}
#removing zeroes
lapply(cars_data, function(x){ length(which(x==0))})
cars_data <- cars_data[ price != 0 ]
cars_data <- cars_data[ powerPS != 0 ]
```

```{r}
#removing blank spaces
lapply(cars_data, function(x){ length(which(x==""))})
cars_data <- cars_data[gearbox != ""]
cars_data <- cars_data[vehicleType != ""]
cars_data <- cars_data[model != ""]
cars_data <- cars_data[fuelType != ""]
cars_data <- cars_data[notRepairedDamage != ""]
```


### 3. Handle outliers

```{r}
#filtering year of registration and price outliers
cars_data <- cars_data[yearOfRegistration > 1900 & yearOfRegistration < 2021 & price < 1.0e+06]
```

```{r}
lapply(cars_data, function(x){ length(which(x==0))})
lapply(cars_data,function(x) { length(which(is.na(x)))})
lapply(cars_data,function(x) { length(which(x == ""))})
```
## EDA

```{r}
ggplot(data=cars_data) + 
    geom_point(mapping= aes(x=powerPS, y=price),color="darkblue") + 
    coord_cartesian(xlim = c(0, 800), ylim = c(0, 100000)) +
    ggtitle("Price vs. PowerPS")
```

### Interpretation
1. From the plot we notice that price is positively correlated with powerPS.
2. Higher powerPS leads to higher listing price, but most were list within 400 range
3. Lower powerPS vehicles tend to have more competitive listing than higher powerPS vehicles.

```{r}
ft <- ggplot(cars_data, aes(x = fuelType))
ft + geom_bar() + ggtitle("Count of Fueltype")
```

### Interpretation
As can be seen from above, the fuel type of most used cars is petrol. From the box plot, we can also see that used cars that used petrol as fuel have a higher range in price. Because cars using petrol can range from luxury sports cars to cheap small cars.

```{r}
boxplot <- ggplot(cars_data, aes(y=price, x = fuelType, col = fuelType)) + geom_boxplot(outlier.color = 'grey', outlier.size = 1.5) + ggtitle("Price vs. fuelType") + scale_y_continuous(trans = 'log10') 
boxplot
```

### Interpretation
1.As can be seen from above, the fuel type of most used cars is petrol. From the box plot, we can also see that used cars that used petrol as fuel have a higher range in price. Because cars using petrol can range from luxury sports cars to cheap small cars.

```{r}
boxplot <- ggplot(cars_data, aes(y=price, x = gearbox, col = gearbox)) + 
    geom_boxplot(outlier.color = 'black', outlier.size = 1.5) + 
    ggtitle("Price vs. Gearbox") + 
    scale_y_continuous(trans = 'log10') 
boxplot
```

### Interpretation
As we can see from the boxplot for Price vs. Gearbox, vehicles with automatic transmission have higher median compared to vehicles with manual transmission. However, manual cars have higher variance in price. This is perhaps because manual vehicles can range from exotic sport cars to very cheap cars.


```{r}
filtered_1 <- cars_data[yearOfRegistration > 1900 & yearOfRegistration < 2021 & price < 1.0e+06,.(price, yearOfRegistration)]

summary(filtered_1)

filtered_1

ggplot(filtered_1,aes(x = yearOfRegistration, y =price)) + 
  geom_point() + 
  geom_rug(col="steelblue",alpha=0.1, size=1.5)


group1 <- filtered_1[,.N,by = yearOfRegistration]

group1

ggplot(group1,aes(x =yearOfRegistration,y=N)) +
  geom_point(color = 'black',size =0.75) +
  geom_line(color = 'red') +
  geom_rug(col="steelblue",alpha=0.1, size=1.5)


filtered_2 <- filtered_1[price < 125000]

ggplot(filtered_2,aes(x = yearOfRegistration, y =price)) + 
  geom_point(color ='red') + 
  geom_rug(col="steelblue",alpha=0.1, size=1.5)
```

### Interpretation
As we can see in the distribution line graph, most of the cars are registered from 1990 to 2010. From the scatter plot, we can observe that in general, the latest cars have normally higher price.

```{r}
boxplot <- ggplot(cars_data, aes(y=price, x = vehicleType, col = vehicleType)) + geom_boxplot(outlier.color = 'black', outlier.size = 1.5) + 
    ggtitle("Price vs. vehicleType") + 
    scale_y_continuous(trans = 'log10')
boxplot
```

### Interpretation
As we can see from the boxplot for Price vs. Vehicle Type, SUVs have higher median compared to other vehicles. As expected, Small Cars have the lowest median of all types of vehicles and Coupe cars have higher range of price.

```{r}
ggplot(cars_data) +
aes(x = kilometer, y = price) +
geom_point(colour = "red")+
xlab("Kilometers")+
ylab("Price")+
ggtitle("Kilometers vs. Price")+
theme(plot.title = element_text(hjust = 0.5))
```
### Interpretation
There is a slight negative relationship between kilometers and price, as kilometers increase price tend to decrease. This makes sense as people prefer cars with lower kilometers driven

## Machine Learning
### 1.Convert categorical data to dummy variables
```{r}
#creating dummy variables for categorical columns
cars_data<- dummy_cols(cars_data, remove_first_dummy=TRUE,remove_selected_columns=TRUE)
```

### 2.Dataset Splitting into Train and Test
```{r message = FALSE, warning = FALSE}
#splitting data into train and test with 70/30 ratio
cars_data[, test:=0]
cars_data[sample(nrow(cars_data), 10014), test:=1]
cars_data.test <- cars_data[test==1]
cars_data.train <- cars_data[test==0]

x1.train <-data.matrix(subset(cars_data.train,select=-(price)))
y.train<-cars_data.train$price
x1.test <-data.matrix(subset(cars_data.test,select=-(price)))
y.test<-cars_data.test$price
```

```{r}
# checking dimensions for original, train, and test sets
dim(cars_data)
dim(cars_data.train)
dim(cars_data.test)
```

## Ridge 

```{r}
fit.ridge <- cv.glmnet(x1.train, y.train, alpha = 0, nfolds = 10)

yhat.train.ridge <- predict(fit.ridge, x1.train, s = fit.ridge$lambda.min)
mse.train.ridge <- mean((y.train - yhat.train.ridge)^2)
RMSE.train.ridge <-sqrt(mse.train.ridge)
RMSE.train.ridge

yhat.test.ridge <- predict(fit.ridge, x1.test, s = fit.ridge$lambda.min)
mse.test.ridge <- mean((y.test - yhat.test.ridge)^2)
RMSE.test.ridge <-sqrt(mse.test.ridge)
RMSE.test.ridge
```
```{r}
#Coefficents shrinkage using the optimal lambda 
ridge <- glmnet(x1.train, y.train, alpha=0)
plot(ridge, xvar = "lambda")
abline(v=log(fit.ridge$lambda.min), col="red")
```

## Lasso

```{r}
# Lasso
fit.lasso <- cv.glmnet(x1.train, y.train, alpha = 1, nfolds = 10)

yhat.train.lasso <- predict(fit.lasso, x1.train, s = fit.lasso$lambda.min)
mse.train.lasso <- mean((y.train - yhat.train.lasso)^2)
RMSE.train.lasso <-sqrt(mse.train.lasso)
RMSE.train.lasso

yhat.test.lasso <- predict(fit.lasso, x1.test, s = fit.lasso$lambda.min)
mse.test.lasso <- mean((y.test - yhat.test.lasso)^2)
RMSE.test.lasso <-sqrt(mse.test.lasso)
RMSE.test.lasso
```

```{r}
#Coefficents shrinkage using the optimal lambda 
lasso <- glmnet(x1.train, y.train, alpha=1)
plot(lasso, xvar = "lambda")
```

### Interpretation
Lasso and Ridge Rationale:
Both Ridge and Lasso models have lower mse tests than OLS models, which means that the penalty lambda is doing a good job penalizing the coefficients. In terms of features with high predictive powers, Kilometer came first, followed by Power PS, then Year of Registration. We found this by analyzing which coefficients pop up when lambda is decreased from greatest to least in our Lasso model.

## OLS
```{r}
# used different train and test sets but same ratio (70/30)
train_ind <- sample(seq_len(nrow(cars_data)), size = floor(0.7 * nrow(cars_data)), replace = FALSE)
options(warn=-1)
train <- cars_data[train_ind, ]
test <- cars_data[-train_ind, ]

f1 <- as.formula(price ~ .)
y.train <- train$price
y.test <- test$price

fit.lm1 <- lm(f1, train)

yhat.train.lm1 <- predict(fit.lm1)
mse.train.lm1 <- mean((y.train - yhat.train.lm1)^2)

RMSE.train<-sqrt(mse.train.lm1)
RMSE.train

yhat.test.lm1 <- predict(fit.lm1, test)
mse.test.lm1 <- mean((y.test - yhat.test.lm1)^2)

RMSE.test<-sqrt(mse.test.lm1)
RMSE.test
```
```{r}
R2_train_ols<- 1 - (sum((y.train-yhat.train.lm1)^2)/sum((y.train-mean(y.train)^2)))
R2_test_ols<- 1 - (sum((y.test-yhat.test.lm1)^2)/sum((y.test-mean(y.test)^2)))

R2_train_ols
R2_test_ols
```
```{r message = FALSE, warning = FALSE}
plot(fit.lm1)
```

## Regression tree

```{r}
set.seed(810)

#splitting the data into a train and a test set with 70/30 ratio
cars_data[, test:=0]
cars_data[sample(nrow(cars_data), nrow(cars_data)*.7), test:=1] 
cars_data.test <- cars_data[test==1] 
cars_data.train <- cars_data[test==0]
cars_data.train.sample.size <- 5000
cars_data.train.sample <- cars_data.train[sample(nrow(cars_data.train), cars_data.train.sample.size)]
#formula
f1 <- as.formula(price ~ .)
#translate the data represented by the formula to a matrix
x1.train.sample <- model.matrix(f1, cars_data.train.sample)[, -1]
y.train <- cars_data.train$price 
y.train.sample <- cars_data.train.sample$price
cars_data.test[, price:=1]  
x1.test <- model.matrix(f1, cars_data.test)[, -1]
y.test <- cars_data.test$price
#Regression tree
fit.tree <- rpart(f1,
                  cars_data.train.sample,
                  control = rpart.control(cp = 0.001))
par(xpd = TRUE) 
plot(fit.tree, compress=TRUE) 
text(fit.tree, use.n=TRUE)
rpart.plot(fit.tree, type = 1)
```
```{r}
#Train MSE
yhat.tree.train <- predict(fit.tree, cars_data.train.sample) 
mse.tree.train <- mean((yhat.tree.train - y.train.sample) ^ 2)
print(mse.tree.train)

yhat.tree.test <- predict(fit.tree, cars_data.test) 
mse.tree.test <- mean((yhat.tree.test - y.test) ^ 2)
print(mse.tree.test)

RMSE.train.tree <- sqrt(mse.tree.train)
RMSE.test.tree <- sqrt(mse.tree.test)
print(RMSE.train.tree)
print(RMSE.test.tree)

R2 <- 1 - (sum((y.train.sample - yhat.tree.train)^2)/sum((y.train.sample-mean(y.train.sample))^2))
print(R2)

```


### Interpretation
We can see that the two most important variables are year of registration and powerPS, which is the horsepower.
Now, imagine that we have a used car that we want to sell. This car is not a Porsche, the powerPS is lower than 129 and the year of registration is between 2003 to 2006. Then this used car can sell for about 3,048 dollars.
But if you own a Porsche car with year of registration before 2006 and powerPS lower than 200, then your car can be sold for 55,000 dollars

## Random Forest

```{r}
library(randomForest)
set.seed(500)
RF_car_data <- setnames(cars_data, "vehicleType_small car", "vehicleType_small_car")
RF_car_data <- setnames(cars_data, "vehicleType_station wagon", "vehicleType_station_wagon")



RF_car_data[, test:=0]
RF_car_data[sample(nrow(RF_car_data), 3000), test:=1]

RF_car_data.test <- RF_car_data[test==1]
RF_car_data.train <- RF_car_data[test==0]

RF_car_data.train.sample.size <- 500
RF_car_data.train.sample <- RF_car_data.train[sample(nrow(RF_car_data.train), RF_car_data.train.sample.size)]


f1 <- as.formula(price ~ .)
x1.train.sample <- model.matrix(f1, RF_car_data.train.sample)[, -1]

y.train <- RF_car_data.train.sample$price
y.train.sample <- RF_car_data.train.sample$price

RF_car_data.test[, p_open:=1]
x1.test <- model.matrix(f1, RF_car_data.test)[, -1]
y.test <- RF_car_data.test$price


fit.rndfor <- randomForest(f1,
                           RF_car_data.train.sample,
                           ntree=200,
                           do.trace=F)

varImpPlot(fit.rndfor)


yhat.rndfor.train <- predict(fit.rndfor, RF_car_data.train.sample)
mse.tree.train <- mean((yhat.rndfor.train - y.train.sample) ^ 2)
rmse.tree.train <- sqrt(mse.tree.train)
print(mse.tree.train)
print(rmse.tree.train)


R2.train <- 1 - (sum((y.train.sample-yhat.rndfor.train)^2)/sum((y.train.sample-mean(y.train.sample))^2))
R2.train



yhat.rndfor.test <- predict(fit.rndfor, x1.test)

mse.tree.test <- mean((yhat.rndfor.test - y.test) ^ 2)
rmse.tree.test <- sqrt(mse.tree.test)

print(mse.tree.test)
print(rmse.tree.test)

R2.test <- 1 - (sum((y.test-yhat.rndfor.test)^2)/sum((y.test-mean(y.test))^2))
R2.test

plot(fit.rndfor)
```

### Interpretation
We can see that OOB Error converges in after 50 iteration of trees. The most important features are YearofRedistration, PowerPS, and Kilometer. Train RMSE is 2091.875, and the Test RMSE is 4913.075, which has huge improvement compare to previous models.

## GBM Boosting
```{r}
library(gbm)
set.seed(800)
#Split dara into test and train 
cars_data[, test:=0]
cars_data[sample(nrow(cars_data), 10000), test:=1] # take 100K random rows and stick them in the test s # now split
cars_data.test <- cars_data[test==1]
cars_data.train <- cars_data[test==0]

cars_data.train.sample.size <- 5000
cars_data.train.sample <- cars_data.train[sample(nrow(cars_data.train), cars_data.train.sample.size)]

#Data preparation
f1 <- as.formula(price ~ .)

x1.train.sample <- model.matrix(f1, cars_data.train.sample)[, -1]
y.train <- cars_data.train$price
y.train.sample <- cars_data.train.sample$price

cars_data.test[, price:=1] # hack so that the following line works 
x1.test <- model.matrix(f1, cars_data.test)[, -1]
y.test <- cars_data.test$price
```

### Boosting 1 (n.trees=100)
```{r message = FALSE, warning = FALSE}
set.seed(800)
fit.btree <- gbm(f1,
                 data = cars_data.train.sample,
                 distribution = "gaussian",
                 n.trees = 100,
                 interaction.depth = 2,
                 shrinkage = 0.001)

yhat.btree <- predict(fit.btree,cars_data.train.sample, n.trees = 100) 
mse.btree <- mean((yhat.btree - y.train.sample) ^ 2) 
print(sqrt(mse.btree))

# compute test MSE
yhat.fit1 <- predict(fit.btree, newdata = cars_data.test, n.trees = 100)
mse.fit1 <- mean((yhat.fit1 - cars_data.test$price) ^ 2)
rmse.fit1 <- sqrt(mse.fit1)
print(rmse.fit1)

```

### Boosting 2 (n.trees=500)
```{r message = FALSE, warning = FALSE}
set.seed(800)
fit.btree2 <- gbm(f1,
                 data = cars_data.train.sample,
                 distribution = "gaussian",
                 n.trees = 500,
                 interaction.depth = 2,
                 cv.folds = 10)

yhat.btree2 <- predict(fit.btree2,cars_data.train.sample, n.trees = 500) 
mse.btree2 <- mean((yhat.btree2 - y.train.sample) ^ 2) 
print(sqrt(mse.btree2))

# compute test MSE
yhat.fit2 <- predict(fit.btree2, newdata = cars_data.test, n.trees = 500)
mse.fit2 <- mean((yhat.fit2 - cars_data.test$price) ^ 2)
rmse.fit2 <- sqrt(mse.fit2)
print(rmse.fit2)
```
```{r message = FALSE, warning = FALSE}
# Plot loss function when n=500 tree added to the ensemble
gbm.perf(fit.btree2, method = "cv")
```
```{r}
# compute min test MSE at 240 iteration
mse.error <- which.min(fit.btree2$cv.error)
mse.error

mse.min <- fit.btree2$cv.error[mse.error]
print(sqrt(mse.min))
```


```{r}
summary(
  fit.btree2,
  cBars = 10,
  method = relative.influence,
  las = 2
  )

relative.influence(fit.btree2)
```
### Interpretation
We plot a loss function of n trees added to the model.Based on the plot we can see that the optimal iteration with minimum cross-validation error when we run the model with 500 trees occurs when n.tree is 492, with lowest RMSE at 8376. Also we found that powerPS, year Of Registration and kilometer are the most relative influence variables.

## Conclusion
1.In summary, we implemented different ML algorithms to predict price of used car prices on ebay for the purpose of providing guidance for future buyers and sellers.

2.According to our EDA, we learned that some patterns and relationships exist among the the features weâ€™ve chosen to our price target variable. For instance, we saw that SUVs tend to have higher median price because they have lower depreciation compared to more common types of cars. 

3.Comparing our test MSEs, Random Forests performed the best, followed by GBM Boosting, Ridge and Lasso, Regression Tree, then OLS. Thus, we can see that the ensemble methods worked best with our dataset since our features do not have a linear relationship with our target variable.
